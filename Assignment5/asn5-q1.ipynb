{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 1:  Implement the Histogram of Curvature Scale\n",
    "\n",
    "Write a function called HoCS that returns a histogram of curvature scale feature vector for a given region.  The inputs to your function should be:\n",
    "\n",
    "- `B`: a binary image that contains exactly one foreground connected component.\n",
    "- `min_scale`: The smallest scale (circle radius) at which to calcluate curvature\n",
    "- `max_scale`: The largest scale (circle radius) at which to calculate curvature\n",
    "- `increment`: The increment at which intermediate curvatures should be calculated (must be a positive integer)\n",
    "- `num_bins`: The number of bins in the histogram of curvature for a single scale (must be a positive integer)\n",
    "\n",
    "Your function should compute a histogram of curvature for each scale, starting at `min_scale` ending at (at most) `max_scale`, and for intermediate scales at increments of `increment`.  For example, if `min_scale`=4 and `max_scale`=20, and `increment`=3, then the function should compute a histogram of curvature for scales 4, 7, 10, 13, 16, and 19.  Each histogram at each scale should have `num_bins` bins.  Curvature must be computed using the normalized area integral invariant method described on Slide 39 of the Topic 9 lecture notes.  \n",
    "\n",
    "Normalize each histogram at each scale.\n",
    "\n",
    "To keep things straightforward, your functions hould only consider the outer perimeter of the input region; ignore the boundaries of holes in the region.\n",
    "\n",
    "After computing the histogram of curvature at each of the specified scales, all of the histograms should be concatenated into a single one-dimensional array (feature vector) and then returned.\n",
    "\n",
    "_Implementation hint:  You can calculate the normalized area integral invariant of each pixel efficiently using linear filtering.  You will find the function `skimage.morphology.disk()` function useful for designing the appropriate filter masks._\n",
    "\n",
    "_Implementation hint:  Most of the heavy lifting here can be done with module functions from `skimage`, `numpy`, and `scipy`.  Many of the functions mentioned in class and in the notes will be useful.  One that we might not have covered, but will be very handy is `numpy.histogram()`.  When you use it, makes sure you specify both the `bins` and `range` optional arguments. Also note that `numpy.histogram()` returns TWO things.  You only need the first one, so make sure you write your function call like this:_\n",
    "\n",
    "`the_histogram, stuff_you_dont_need = np.histogram(...)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your HoCS function here\n",
    "import numpy as np\n",
    "import skimage.segmentation as seg\n",
    "import skimage.morphology as morph\n",
    "import os as os\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def HoCS(B, min_scale, max_scale, increment, num_bins):\n",
    "    '''\n",
    "    Computes a histogram of curvature scale for the shape in the binary image B.  \n",
    "    Boundary fragments due to holes are ignored.\n",
    "    :param B: A binary image consisting of a single foreground connected component.\n",
    "    :param min_scale: smallest scale to consider (minimum 1)\n",
    "    :param max_scale: largest scale to consider (max_scale > min_scale)\n",
    "    :param increment:  increment on which to compute scales between min_scale and max_scale\n",
    "    :param num_bins: number of bins for the histogram at each scale\n",
    "    :return: 1D array of histograms concatenated together in order of increasing scale.\n",
    "    '''\n",
    "    \n",
    "    assert min_scale <= max_scale\n",
    "    \n",
    "    B_no_hole = ndimage.binary_fill_holes(B).astype(int)\n",
    "    B_no_hole = np.pad(B_no_hole, pad_width=max_scale, mode='constant', constant_values=(False))\n",
    "    bp = np.where(seg.find_boundaries(B_no_hole, connectivity=1, mode='inner') > 0)\n",
    "    bp = np.transpose(np.vstack(bp))\n",
    "    \n",
    "    conca_hist = np.array([])\n",
    "    start = min_scale\n",
    "    while start <= max_scale:\n",
    "        hist = FindHist(bp, B_no_hole, start, num_bins)\n",
    "        conca_hist = np.concatenate((conca_hist, hist))\n",
    "        start = start + increment\n",
    "        \n",
    "    return conca_hist\n",
    "\n",
    "def FindHist(bdry, B, scale, num_bins):\n",
    "    disc = morph.disk(scale)\n",
    "    disc_area = sum(map(sum, disc))\n",
    "    areas = []\n",
    "    for row, col in bdry:\n",
    "        areas.append(sum(map(sum, np.logical_and(B[row - scale : row + scale + 1, col - scale : col + scale + 1], disc))) * 1.0 / disc_area)\n",
    "    hist, bin_edges = np.histogram(areas, range=(0.0, 1.0))\n",
    "                                   \n",
    "    return (hist / len(areas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Test your HoCS function.\n",
    "\n",
    "Run HoCS on `threshimage_0001.png` from the ground truth for assignment 3.  Use `min_scale=5`, `max_scale=25`, `increment=10`, `num_bins=10`.  Plot the resulting feature vector as a bar graph.  Set the y-axis limits to be between 0.0 and 1.0.  You should get a result that matches the sample output in the assignment description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Paths for folders -- ground truth images\n",
    "gt_path = os.path.join('.', 'leaftraining')\n",
    "gtfilename = 'threshimage_0001.png'\n",
    "I_gt = io.imread(os.path.join(gt_path, gtfilename))\n",
    "\n",
    "conca_hist = HoCS(I_gt > 0, min_scale=5, max_scale=25, increment=10, num_bins=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.ylim((0,1))\n",
    "plt.bar(np.arange(len(conca_hist)), conca_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate training features.\n",
    "\n",
    "Use your function from Step 1 to compute the HoCS feature for each of the training images.  Use them to train a k-nearest neigbour classifier.  It is up to you to determine the parameters for the HoCS feature such as `min_scale`, `max_scale`, etc. to maximize the classification rate.  This will require some experimentation.  Slides 19-21 of Topic 12 lecture notes will be helpful here.\n",
    "\n",
    "Also generate the training labels here (a column-array of numbers indicating which descriptors belong to each class, e.g. use values 1,2,3 to indicate class 1, 2, and 3.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./leaftraining/threshimage_0001.png\n",
      "./leaftraining/threshimage_0002.png\n",
      "./leaftraining/threshimage_0005.png\n",
      "./leaftraining/threshimage_0007.png\n",
      "./leaftraining/threshimage_0009.png\n",
      "./leaftraining/threshimage_0010.png\n",
      "./leaftraining/threshimage_0011.png\n",
      "./leaftraining/threshimage_0015.png\n",
      "./leaftraining/threshimage_0018.png\n",
      "./leaftraining/threshimage_0019.png\n",
      "./leaftraining/threshimage_0078.png\n",
      "./leaftraining/threshimage_0080.png\n",
      "./leaftraining/threshimage_0089.png\n",
      "./leaftraining/threshimage_0090.png\n",
      "./leaftraining/threshimage_0099.png\n",
      "./leaftraining/threshimage_0100.png\n",
      "./leaftraining/threshimage_0104.png\n",
      "./leaftraining/threshimage_0105.png\n",
      "./leaftraining/threshimage_0110.png\n",
      "./leaftraining/threshimage_0113.png\n",
      "./leaftraining/threshimage_0132.png\n",
      "./leaftraining/threshimage_0160.png\n",
      "./leaftraining/threshimage_0161.png\n",
      "./leaftraining/threshimage_0162.png\n",
      "./leaftraining/threshimage_0163.png\n",
      "./leaftraining/threshimage_0165.png\n",
      "./leaftraining/threshimage_0166.png\n",
      "./leaftraining/threshimage_0171.png\n",
      "./leaftraining/threshimage_0174.png\n",
      "./leaftraining/threshimage_0175.png\n"
     ]
    }
   ],
   "source": [
    "import sklearn.neighbors as neigh\n",
    "import os as os\n",
    "\n",
    "# use os.walk() as in previous assignments to process the training images.\n",
    "# Paths for folders -- ground truth images\n",
    "gt_path = os.path.join('.', 'leaftraining')\n",
    "\n",
    "\n",
    "train_X = []\n",
    "train_y = []\n",
    "count_train_img = 0\n",
    "# Iterate over all files in the original images folder\n",
    "for root, dirs, files in os.walk(gt_path):\n",
    "    for filename in files:\n",
    "        # ignore files that are not PNG files.\n",
    "        if filename[-4:] != '.png':\n",
    "            continue\n",
    "        \n",
    "        # concatenate variable root with filename to get the path to an input file.\n",
    "        fname = os.path.join(root, filename)\n",
    "        print(fname)\n",
    "        I = io.imread(fname)\n",
    "        count_train_img = count_train_img + 1\n",
    "        train_X.append(HoCS(I > 0, min_scale=5, max_scale=25, increment=10, num_bins=10))\n",
    "        if (count_train_img <= 10):\n",
    "            train_y.append(1)\n",
    "        elif (count_train_img <= 20):\n",
    "            train_y.append(2)\n",
    "        else:\n",
    "            train_y.append(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train the KNN classifier using the feature vectors from the training images.\n",
    "\n",
    "You have another opportunity here to optimize parameters.  You can experiment with the options for the KNN classifier (in partiuclar n_neighbors) to try to obtain better classification rates.  But you won't really be able to do this until after step 6, so just use default parameters to start with. \n",
    "\n",
    "Hint: The steps in this notebook are broken up the way they are so that you can adjust the parameters of training the classifier and then go and perform the classfication without having to re-run the calculation of the features in steps 3 and 5.  You can adjust the parameters here in step 4, and then go and re-run the test set in Step 6 without running step 5 over again -- which is good because step 5 will take a while to run.  Of course you will have to recalculate the features each time you restart PyCharm or the Jupyter Notebook server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the KNN classifier\n",
    "ngh = neigh.KNeighborsClassifier(n_neighbors=1)\n",
    "ngh.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Calculate the testing features.\n",
    "\n",
    "Compute the HoCS features for all of the testing images.  Use the same HoCS parameters you did in Step 3.  Also generate class labels for the testing image descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./leaftesting/image_0003.png\n",
      "./leaftesting/image_0004.png\n",
      "./leaftesting/image_0009.png\n",
      "./leaftesting/image_0012.png\n",
      "./leaftesting/image_0015.png\n",
      "./leaftesting/image_0016.png\n",
      "./leaftesting/image_0017.png\n",
      "./leaftesting/image_0020.png\n",
      "./leaftesting/image_0021.png\n",
      "./leaftesting/image_0023.png\n",
      "./leaftesting/image_0024.png\n",
      "./leaftesting/image_0025.png\n",
      "./leaftesting/image_0026.png\n",
      "./leaftesting/image_0027.png\n",
      "./leaftesting/image_0029.png\n",
      "./leaftesting/image_0030.png\n",
      "./leaftesting/image_0031.png\n",
      "./leaftesting/image_0033.png\n",
      "./leaftesting/image_0034.png\n",
      "./leaftesting/image_0035.png\n",
      "./leaftesting/image_0036.png\n",
      "./leaftesting/image_0037.png\n",
      "./leaftesting/image_0038.png\n",
      "./leaftesting/image_0039.png\n",
      "./leaftesting/image_0040.png\n",
      "./leaftesting/image_0041.png\n",
      "./leaftesting/image_0043.png\n",
      "./leaftesting/image_0044.png\n",
      "./leaftesting/image_0045.png\n",
      "./leaftesting/image_0046.png\n",
      "./leaftesting/image_0047.png\n",
      "./leaftesting/image_0048.png\n",
      "./leaftesting/image_0049.png\n",
      "./leaftesting/image_0050.png\n",
      "./leaftesting/image_0051.png\n",
      "./leaftesting/image_0052.png\n",
      "./leaftesting/image_0053.png\n",
      "./leaftesting/image_0054.png\n",
      "./leaftesting/image_0055.png\n",
      "./leaftesting/image_0056.png\n",
      "./leaftesting/image_0057.png\n",
      "./leaftesting/image_0058.png\n",
      "./leaftesting/image_0059.png\n",
      "./leaftesting/image_0060.png\n",
      "./leaftesting/image_0061.png\n",
      "./leaftesting/image_0062.png\n",
      "./leaftesting/image_0063.png\n",
      "./leaftesting/image_0064.png\n",
      "./leaftesting/image_0065.png\n",
      "./leaftesting/image_0066.png\n",
      "./leaftesting/image_0067.png\n",
      "./leaftesting/image_0068.png\n",
      "./leaftesting/image_0070.png\n",
      "./leaftesting/image_0071.png\n",
      "./leaftesting/image_0072.png\n",
      "./leaftesting/image_0075.png\n",
      "./leaftesting/image_0076.png\n",
      "./leaftesting/image_0077.png\n",
      "./leaftesting/image_0081.png\n",
      "./leaftesting/image_0082.png\n",
      "./leaftesting/image_0083.png\n",
      "./leaftesting/image_0085.png\n",
      "./leaftesting/image_0086.png\n",
      "./leaftesting/image_0087.png\n",
      "./leaftesting/image_0088.png\n",
      "./leaftesting/image_0091.png\n",
      "./leaftesting/image_0092.png\n",
      "./leaftesting/image_0093.png\n",
      "./leaftesting/image_0095.png\n",
      "./leaftesting/image_0096.png\n",
      "./leaftesting/image_0097.png\n",
      "./leaftesting/image_0101.png\n",
      "./leaftesting/image_0103.png\n",
      "./leaftesting/image_0111.png\n",
      "./leaftesting/image_0118.png\n",
      "./leaftesting/image_0125.png\n",
      "./leaftesting/image_0126.png\n",
      "./leaftesting/image_0127.png\n",
      "./leaftesting/image_0128.png\n",
      "./leaftesting/image_0129.png\n",
      "./leaftesting/image_0130.png\n",
      "./leaftesting/image_0131.png\n",
      "./leaftesting/image_0132.png\n",
      "./leaftesting/image_0133.png\n",
      "./leaftesting/image_0134.png\n",
      "./leaftesting/image_0136.png\n",
      "./leaftesting/image_0138.png\n",
      "./leaftesting/image_0139.png\n",
      "./leaftesting/image_0140.png\n",
      "./leaftesting/image_0141.png\n",
      "./leaftesting/image_0142.png\n",
      "./leaftesting/image_0143.png\n",
      "./leaftesting/image_0144.png\n",
      "./leaftesting/image_0145.png\n",
      "./leaftesting/image_0146.png\n",
      "./leaftesting/image_0147.png\n",
      "./leaftesting/image_0148.png\n",
      "./leaftesting/image_0149.png\n",
      "./leaftesting/image_0150.png\n",
      "./leaftesting/image_0151.png\n",
      "./leaftesting/image_0152.png\n",
      "./leaftesting/image_0153.png\n",
      "./leaftesting/image_0154.png\n",
      "./leaftesting/image_0155.png\n",
      "./leaftesting/image_0157.png\n",
      "./leaftesting/image_0158.png\n",
      "./leaftesting/image_0159.png\n",
      "./leaftesting/image_0160.png\n",
      "./leaftesting/image_0161.png\n",
      "./leaftesting/image_0162.png\n",
      "./leaftesting/image_0163.png\n",
      "./leaftesting/image_0164.png\n",
      "./leaftesting/image_0165.png\n",
      "./leaftesting/image_0166.png\n",
      "./leaftesting/image_0167.png\n",
      "./leaftesting/image_0168.png\n",
      "./leaftesting/image_0171.png\n",
      "./leaftesting/image_0173.png\n",
      "./leaftesting/image_0174.png\n",
      "./leaftesting/image_0175.png\n",
      "./leaftesting/image_0177.png\n",
      "./leaftesting/image_0178.png\n",
      "./leaftesting/image_0179.png\n",
      "./leaftesting/image_0180.png\n",
      "./leaftesting/image_0181.png\n",
      "./leaftesting/image_0182.png\n",
      "./leaftesting/image_0184.png\n",
      "./leaftesting/image_0185.png\n",
      "./leaftesting/image_0186.png\n"
     ]
    }
   ],
   "source": [
    "# again use os.walk() to process the testing images\n",
    "# use os.walk() as in previous assignments to process the training images.\n",
    "# Paths for folders -- ground truth images\n",
    "test_path = os.path.join('.', 'leaftesting')\n",
    "\n",
    "\n",
    "test_X = []\n",
    "test_y = []\n",
    "count_test_img = 0\n",
    "# Iterate over all files in the original images folder\n",
    "for root, dirs, files in os.walk(test_path):\n",
    "    for filename in files:\n",
    "        # ignore files that are not PNG files.\n",
    "        if filename[-4:] != '.png':\n",
    "            continue\n",
    "        \n",
    "        # concatenate variable root with filename to get the path to an input file.\n",
    "        fname = os.path.join(root, filename)\n",
    "        print(fname)\n",
    "        I = io.imread(fname)\n",
    "        count_test_img = count_test_img + 1\n",
    "        test_X.append(HoCS(I > 0, min_scale=5, max_scale=25, increment=10, num_bins=10))\n",
    "        if (count_test_img <= 50):\n",
    "            test_y.append(1)\n",
    "        elif (count_test_img <= 77):\n",
    "            test_y.append(2)\n",
    "        else:\n",
    "            test_y.append(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Classfiy the testing features.\n",
    "\n",
    "Classify the testing image features.\n",
    "\n",
    "Determine the classification rate and the confusion matrix by comparing the results of the classifier to the true class labels for each image.  \n",
    "\n",
    "Print out the filenames of incorrectly classified images.\n",
    "\n",
    "Print the confusion matrix (you don't have to print the row/column indicies as in the example in the assignment description), just the rows and columns of the matrix itself.\n",
    "\n",
    "Print the correct classification rate.\n",
    "\n",
    "It should be very easy to get a classficiation rate more than 90%; with care you should be able to get as much as 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification rate was: 0.976744186047\n",
      "--------------------------------------------------\n",
      "The confusion matrix was: \n",
      "[[48  0  2]\n",
      " [ 0 27  0]\n",
      " [ 1  0 51]]\n",
      "--------------------------------------------------\n",
      "Incorrectly classified images was: \n",
      "image_0060.png\n",
      "image_0064.png\n",
      "image_0186.png\n"
     ]
    }
   ],
   "source": [
    "# Write your code for Step 6 here.\n",
    "test_y_pred = ngh.predict(test_X)\n",
    "test_y = np.array(test_y)\n",
    "\n",
    "err_pred = test_y - test_y_pred\n",
    "err_pred_abs = abs(err_pred)\n",
    "cls_rate = sum(np.logical_not(err_pred_abs)) * 1.0 / test_y.shape[0]\n",
    "print(\"The classification rate was: %.12f\" % cls_rate)\n",
    "print(\"--------------------------------------------------\")\n",
    "conf_mx = np.zeros((3,3),dtype=int)\n",
    "err_idx = np.where(err_pred)\n",
    "err_idx = err_idx[0]\n",
    "for i in err_idx:\n",
    "    if (i <= 49):\n",
    "        conf_mx[0, (0 - err_pred[i])] = conf_mx[0, (0 - err_pred[i])] + 1\n",
    "    elif (i <= 76):\n",
    "        conf_mx[1, (1 - err_pred[i])] = conf_mx[1, (1 - err_pred[i])] + 1     \n",
    "    else:\n",
    "        conf_mx[2, (2 - err_pred[i])] = conf_mx[2, (2 - err_pred[i])] + 1\n",
    "conf_mx[0,0] = 50 - sum(conf_mx[0,:])\n",
    "conf_mx[1,1] = 27 - sum(conf_mx[1,:])\n",
    "conf_mx[2,2] = 52 - sum(conf_mx[2,:])\n",
    "print(\"The confusion matrix was: \")\n",
    "print(conf_mx)\n",
    "print(\"--------------------------------------------------\")\n",
    "# use os.walk() as in previous assignments to process the training images.\n",
    "# Paths for folders -- ground truth images\n",
    "print(\"Incorrectly classified images was: \")\n",
    "test_path = os.path.join('.', 'leaftesting')\n",
    "\n",
    "count_test_img = 0\n",
    "err_i = 0\n",
    "# Iterate over all files in the original images folder\n",
    "for root, dirs, files in os.walk(test_path):\n",
    "    for filename in files:\n",
    "        # ignore files that are not PNG files.\n",
    "        if filename[-4:] != '.png':\n",
    "            continue\n",
    "        \n",
    "        # concatenate variable root with filename to get the path to an input file.\n",
    "        count_test_img = count_test_img + 1\n",
    "        if err_i < 3:\n",
    "            if count_test_img == (err_idx[err_i] + 1):\n",
    "                print(filename)\n",
    "                err_i = err_i + 1\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Reflections\n",
    "\n",
    "Answer the following questions right here in this block:\n",
    "\n",
    "- Discuss your HoCS parameters and how you arrived at them.  Why did you choose the scales and number of histogram bins that you did?  Are there other values that work just as well?   Likely you tested other HoCS parameters that resulted in worse performance before finding the ones that worked best -- what were some of them and why do you think the performance was worse?\n",
    "\n",
    "\t_Your answer:_ I just used the default scales and number of histogram bins to achieve my classification rate.The scales should be large enough to exclude small apexs introduced by segmentation errors but not too large so that computation won't take too long. So I think 5-25 pixels was a good start. Besides, if the number of histogram bins is too small, the KNN classifier may underfit our case. And if the number of histogram bins is too large, the KNN classifier may overfit our case. I may try min_scale=5, max_scale=35, increment=10, num_bins=10 if I didn't achieve desired classification rates.\n",
    "    \n",
    "\n",
    "- Discuss your choice of KNN classifier parameters and how you arrived at them (think about the same types of questions as in the previous point).\n",
    "\n",
    "\t_Your answer:_ I changed the n_neighbors parameter of KNN classifier from 1 to 5 and found that n_neighbors = 1 was the only value that made the classification rate higher than 95%. In this case, each class of leaves have same number of apexs. And histograms of curvature can capture that feature very well. So we can identify the label of a leaf very well by just one neighbor. The more neighbors we choose, we are more likely to introduce noises and errors, which lead to lower misclassification rates.\n",
    "    \n",
    "\n",
    "- Discuss the misclassified images.  Were there any classes that were particularly difficult to distinguish?  Is there anything unusual about any of the misclassified images that would cuase them to be misclassified?  If so, explain\n",
    "\n",
    "\t_Your answer:_ There are 2 images of class 1 and 1 iamge of class 3 being misclassified. So I think there were no classes that were particularly difficult to distinguish. I noticed that the leaf of image_0186.png had a big hole connected to its background. So it was impossible to remove this hole during preprocessing. Besides, this hole introduced 2 big apexs so that this image was classified to class 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
