{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os as os\n",
    "import time\n",
    "import tarfile\n",
    "import pickle\n",
    "\n",
    "cifar10_dataset_tar_gz = 'cifar-10-python.tar.gz'\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# untar cifar10 dataset\n",
    "if not os.path.isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(cifar10_dataset_tar_gz) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    enc = OneHotEncoder(n_values = 10)\n",
    "    x = np.array(x).reshape(-1, 1)\n",
    "    \n",
    "    return enc.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='latin1')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess Training, Validation, and Testing Data\n",
    "temp_train_features = []\n",
    "temp_train_labels = []\n",
    "temp_valid_features = []\n",
    "temp_valid_labels = []\n",
    "test_proportion = 0.8\n",
    "\n",
    "# original data\n",
    "for i in range(1,6):\n",
    "    batch = unpickle(cifar10_dataset_folder_path + '/data_batch_' + str(i))\n",
    "    \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    test_count = int(len(features) * test_proportion)\n",
    "    \n",
    "    temp_train_features.extend(features[:test_count])\n",
    "    temp_train_labels.extend(labels[:test_count]) \n",
    "    temp_valid_features.extend(features[test_count:])\n",
    "    temp_valid_labels.extend(labels[test_count:])\n",
    "    \n",
    "'''\n",
    "# add data Augmentation: flip train images left and right \n",
    "for i in range(1,6):\n",
    "    batch = unpickle(cifar10_dataset_folder_path + '/data_batch_' + str(i))\n",
    "    \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    test_count = int(len(features) * test_proportion)\n",
    "    \n",
    "    temp_train_features.extend(np.flip(features[:test_count], axis=2))\n",
    "    temp_train_labels.extend(labels[:test_count]) \n",
    "\n",
    "# add data Augmentation: flip train images up and down\n",
    "for i in range(1,6):\n",
    "    batch = unpickle(cifar10_dataset_folder_path + '/data_batch_' + str(i))\n",
    "    \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    test_count = int(len(features) * test_proportion)\n",
    "    \n",
    "    temp_train_features.extend(np.flip(features[:test_count], axis=1))\n",
    "    temp_train_labels.extend(labels[:test_count]) \n",
    "'''\n",
    "\n",
    "train_features = np.array(temp_train_features) / 255\n",
    "train_labels = one_hot_encode(np.array(temp_train_labels))\n",
    "valid_features = np.array(temp_valid_features) / 255\n",
    "valid_labels = one_hot_encode(np.array(temp_valid_labels))\n",
    "\n",
    "batch = unpickle(cifar10_dataset_folder_path + '/test_batch')\n",
    "features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "labels = batch['labels']\n",
    "\n",
    "test_features = np.array(features) / 255\n",
    "test_labels = one_hot_encode(np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides):\n",
    "    weight = tf.Variable(tf.truncated_normal([*conv_ksize, x_tensor.shape[3].value, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, *conv_strides, 1], padding = \"SAME\")\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "\n",
    "    return conv_layer\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    conv_layer = conv2d(x_tensor, conv_num_outputs, conv_ksize, conv_strides)\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, *pool_ksize, 1], strides=[1, *pool_strides, 1], padding='SAME')\n",
    "\n",
    "    return conv_layer\n",
    "\n",
    "def flatten(x_tensor):\n",
    "    flattened_image_size = 1\n",
    "    for shape in x_tensor.get_shape().as_list()[1:]:\n",
    "        flattened_image_size = flattened_image_size * shape\n",
    "        \n",
    "    return tf.reshape(x_tensor, [-1, flattened_image_size])\n",
    "\n",
    "def fully_connect(x_tensor, num_outputs, keep_prob):\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    fully_connect_layer = tf.matmul(x_tensor, weight)\n",
    "    fully_connect_layer = tf.add(fully_connect_layer, bias)\n",
    "    fully_connect_layer = tf.nn.relu(fully_connect_layer)\n",
    "    fully_connect_layer = tf.nn.dropout(fully_connect_layer, keep_prob)\n",
    "    \n",
    "    return fully_connect_layer\n",
    "\n",
    "def output(x_tensor, num_outputs):\n",
    "    weight = tf.Variable(tf.truncated_normal([x_tensor.shape[1].value, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    output_layer = tf.matmul(x_tensor, weight)\n",
    "    output_layer = tf.add(output_layer, bias)\n",
    "    \n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet(x, keep_prob):\n",
    "    # convolutional layer 1\n",
    "    x = conv2d_maxpool(x, 32, conv_ksize = (4, 4), conv_strides = (2, 2), pool_ksize = (4, 4), pool_strides = (2,2))\n",
    "    \n",
    "    # convolutional layer 2, comment this layer if shallow CNN\n",
    "    x = conv2d_maxpool(x, 128, conv_ksize = (4, 4), conv_strides = (2, 2), pool_ksize = (4, 4), pool_strides = (2,2))\n",
    "    \n",
    "    # convolutioanl layer 3, comment this layer if shallow CNN\n",
    "    x = conv2d(x, 256, conv_ksize = (4, 4), conv_strides = (2, 2))\n",
    "    \n",
    "    # convolutional layer 4, comment this layer if shallow CNN\n",
    "    x = conv2d(x, 256, conv_ksize = (4, 4), conv_strides = (2, 2))\n",
    "    \n",
    "    # convolutional layer 5, comment this layer if shallow CNN\n",
    "    x = conv2d_maxpool(x, 128, conv_ksize = (4, 4), conv_strides = (2, 2), pool_ksize = (4, 4), pool_strides = (2,2))\n",
    "\n",
    "    # flatten tensor\n",
    "    x = flatten(x)    \n",
    "\n",
    "    # fully connected layer 1, comment this layer if shallow CNN\n",
    "    x = fully_connect(x, 1024, keep_prob)\n",
    "    \n",
    "    # fully connected layer 2\n",
    "    x = fully_connect(x, 1024, keep_prob)\n",
    "    \n",
    "    #  fully connected layer 3 (output layer)\n",
    "    x = output(x, 10)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-e517365ecd56>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove previous settings\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders for inputs, outputs, and keep probability\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# model\n",
    "logits = alexnet(x, keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_error_rate_without_data_aug = []\n",
    "#valid_error_rate_with_left_right_data = []\n",
    "#valid_error_rate_with_up_down_data = []\n",
    "#valid_error_rate_shallow = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameter\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Epoch:  1 Loss: 1.38694155 Validation error rate: 0.58109999\n",
      "Epoch:  2 Loss: 1.08432484 Validation error rate: 0.50690001\n",
      "Epoch:  3 Loss: 0.95274985 Validation error rate: 0.47899997\n",
      "Epoch:  4 Loss: 1.00005651 Validation error rate: 0.45169997\n",
      "Epoch:  5 Loss: 0.89519370 Validation error rate: 0.42479998\n",
      "Epoch:  6 Loss: 0.89223546 Validation error rate: 0.43519998\n",
      "Epoch:  7 Loss: 0.72558546 Validation error rate: 0.39600003\n",
      "Epoch:  8 Loss: 0.75619006 Validation error rate: 0.40890002\n",
      "Epoch:  9 Loss: 0.55213779 Validation error rate: 0.40020001\n",
      "Epoch: 10 Loss: 0.51213378 Validation error rate: 0.37519997\n",
      "Epoch: 11 Loss: 0.40274268 Validation error rate: 0.36849999\n",
      "Epoch: 12 Loss: 0.31951755 Validation error rate: 0.37199998\n",
      "Epoch: 13 Loss: 0.32067901 Validation error rate: 0.37239999\n",
      "Epoch: 14 Loss: 0.31022343 Validation error rate: 0.37279999\n",
      "Epoch: 15 Loss: 0.27726069 Validation error rate: 0.36739999\n",
      "Epoch: 16 Loss: 0.24779756 Validation error rate: 0.36129999\n",
      "Epoch: 17 Loss: 0.27677998 Validation error rate: 0.36870003\n",
      "Epoch: 18 Loss: 0.32374787 Validation error rate: 0.36449999\n",
      "Epoch: 19 Loss: 0.22386312 Validation error rate: 0.36400002\n",
      "Epoch: 20 Loss: 0.29748681 Validation error rate: 0.37089998\n",
      "Epoch: 21 Loss: 0.27256033 Validation error rate: 0.36669999\n",
      "Epoch: 22 Loss: 0.17152333 Validation error rate: 0.35430002\n",
      "Epoch: 23 Loss: 0.19436544 Validation error rate: 0.36720002\n",
      "Epoch: 24 Loss: 0.17516351 Validation error rate: 0.37599999\n",
      "Epoch: 25 Loss: 0.22487770 Validation error rate: 0.35970002\n",
      "Epoch: 26 Loss: 0.17746826 Validation error rate: 0.35960001\n",
      "Epoch: 27 Loss: 0.21307200 Validation error rate: 0.38010001\n",
      "Epoch: 28 Loss: 0.16407341 Validation error rate: 0.36989999\n",
      "Epoch: 29 Loss: 0.18213040 Validation error rate: 0.36589998\n",
      "Epoch: 30 Loss: 0.13427648 Validation error rate: 0.36720002\n",
      "Epoch: 31 Loss: 0.15760627 Validation error rate: 0.37769997\n",
      "Epoch: 32 Loss: 0.11083353 Validation error rate: 0.36750001\n",
      "Epoch: 33 Loss: 0.09017324 Validation error rate: 0.36699998\n",
      "Epoch: 34 Loss: 0.10844964 Validation error rate: 0.37239999\n",
      "Epoch: 35 Loss: 0.09005359 Validation error rate: 0.36669999\n",
      "Epoch: 36 Loss: 0.11326711 Validation error rate: 0.36040002\n",
      "Epoch: 37 Loss: 0.09744322 Validation error rate: 0.36970001\n",
      "Epoch: 38 Loss: 0.08877856 Validation error rate: 0.36870003\n",
      "Epoch: 39 Loss: 0.07443226 Validation error rate: 0.38239998\n",
      "Epoch: 40 Loss: 0.05273611 Validation error rate: 0.36220002\n",
      "Training ends\n",
      "Testing error rate : 0.36837421\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "start_time = time.clock()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # training alexnet\n",
    "    for epoch in range(epochs):\n",
    "        for start in range(0, len(train_features), batch_size):\n",
    "            end = min(start + batch_size, len(train_features))\n",
    "            batch_features = train_features[start:end]\n",
    "            batch_labels = train_labels[start:end]\n",
    "            sess.run(optimizer, feed_dict={x: batch_features, y: batch_labels, keep_prob: keep_probability})\n",
    "            \n",
    "        # validation error rate on validation set\n",
    "        loss = sess.run(cost, feed_dict={x: batch_features, y: batch_labels, keep_prob: 1.})\n",
    "        valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "        valid_error_rate_without_data_aug.append(1 - valid_acc)\n",
    "        #valid_error_rate_with_left_right_data.append(1 - valid_acc)\n",
    "        #valid_error_rate_with_up_down_data.append(1 - valid_acc)\n",
    "        #valid_error_rate_shallow.append(1 - valid_acc)\n",
    "        print(\"Epoch: %2d Loss: %.8f Validation error rate: %.8f\" % (epoch + 1, loss, 1 - valid_acc))\n",
    "    \n",
    "    end_time = time.clock()\n",
    "    print(\"Training ends\")\n",
    "    # validation error rate on test set\n",
    "    test_batch_acc_total = 0\n",
    "    test_batch_count = 0\n",
    "    \n",
    "    for start in range(0, len(test_features), batch_size):\n",
    "        end = min(start + batch_size, len(test_features))\n",
    "        test_feature_batch = test_features[start:end]\n",
    "        test_label_batch = test_labels[start:end]\n",
    "        test_batch_acc_total += sess.run(accuracy, feed_dict={x: test_feature_batch, y: test_label_batch, keep_prob: 1.0})\n",
    "        test_batch_count += 1\n",
    "        \n",
    "    print('Testing error rate : %.8f' % (1 - test_batch_acc_total/test_batch_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
